"""
Hyperparameter Tuning for MLP using Keras Tuner.
(Read-Only Mode: This script outputs best params but saves NOTHING to disk)
"""

import os
import pandas as pd
import numpy as np
import tensorflow as tf
import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight

# ä¿æŒéšæœºæ€§ä¸€è‡´
tf.random.set_seed(42)
np.random.seed(42)

def load_data_for_tuning():
    """Load data (Read-Only)."""
    # æ—¢ç„¶ä¸å¼•ç”¨utilsäº†ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ç›¸å¯¹è·¯å¾„è¯»å–
    data_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'processed_data.csv')

    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data not found at {data_path}")

    print(f"Loading data from: {data_path}")
    df = pd.read_csv(data_path)
    target = 'Attrition'

    # å¼ºåˆ¶ç±»å‹è½¬æ¢
    X = df.drop(columns=[target]).values.astype('float32')
    y = df[target].values.astype('float32')
    return X, y

def build_hypermodel(hp):
    """
    Builds a model with hyperparameters to tune.
    """
    model = Sequential()

    # æ³¨æ„ï¼šKeras Tuner ä¼šè‡ªåŠ¨å¤„ç† Input shapeï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥è®©ç¬¬ä¸€å±‚ Dense è‡ªåŠ¨æ¨æ–­

    # 1. è°ƒä¼˜ï¼šéšè—å±‚çš„æ•°é‡ (1 åˆ° 3 å±‚)
    for i in range(hp.Int('num_layers', 1, 3)):
        model.add(Dense(
            # 2. è°ƒä¼˜ï¼šæ¯ä¸€å±‚çš„ç¥ç»å…ƒæ•°é‡
            units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),
            activation='relu'
        ))

        # 3. è°ƒä¼˜ï¼šæ˜¯å¦ä½¿ç”¨ BatchNormalization
        if hp.Boolean('batch_norm'):
            model.add(BatchNormalization())

        # 4. è°ƒä¼˜ï¼šDropout ç‡
        model.add(Dropout(rate=hp.Float('dropout', 0.1, 0.5, step=0.1)))

    # è¾“å‡ºå±‚
    model.add(Dense(1, activation='sigmoid'))

    # 5. è°ƒä¼˜ï¼šå­¦ä¹ ç‡
    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )
    return model

def run_tuning():
    print("\n" + "=" * 80)
    print("STARTING HYPERPARAMETER TUNING (READ-ONLY MODE)")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    X, y = load_data_for_tuning()

    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)

    # è®¡ç®—ç±»åˆ«æƒé‡
    class_weights = class_weight.compute_class_weight(
        class_weight='balanced',
        classes=np.unique(y_train),
        y=y_train
    )
    class_weight_dict = dict(enumerate(class_weights))

    # 2. åˆå§‹åŒ– Tuner
    # æ³¨æ„ï¼šdirectory='kt_dir' æ˜¯ä¸ºäº†å­˜æ”¾æœç´¢è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ–‡ä»¶ï¼Œä¸ä¼šè¦†ç›–ä½ çš„æ¨¡å‹æ–‡ä»¶
    tuner = kt.Hyperband(
        build_hypermodel,
        objective=kt.Objective("val_auc", direction="max"),
        max_epochs=50,
        factor=3,
        directory='kt_dir',
        project_name='hr_attrition_tuning'
    )

    # 3. å¼€å§‹æœç´¢
    print("\nSearching for best hyperparameters...")
    stop_early = EarlyStopping(monitor='val_loss', patience=5)

    tuner.search(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=50,
        callbacks=[stop_early],
        class_weight=class_weight_dict,
        verbose=1
    )

    # 4. è·å–å¹¶æ‰“å°æœ€ä½³è¶…å‚æ•°
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

    print("\n" + "=" * 80)
    print("ğŸ† BEST HYPERPARAMETERS FOUND ğŸ†")
    print("-" * 80)
    print(f"Number of layers: {best_hps.get('num_layers')}")
    print(f"Learning rate:    {best_hps.get('learning_rate')}")
    print(f"Batch Norm:       {best_hps.get('batch_norm')}")
    print(f"Dropout Rate:     {best_hps.get('dropout')}")
    print("-" * 40)

    for i in range(best_hps.get('num_layers')):
        print(f"Layer {i} Units:      {best_hps.get(f'units_{i}')}")

    print("=" * 80 + "\n")
    print("âœ… Tuning finished! You can now manually copy these parameters to mlp.py.")

if __name__ == "__main__":
    run_tuning()